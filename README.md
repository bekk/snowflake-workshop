# Workshop: Introduksjon til Snowflake ‚ùÑÔ∏è

Velkommen til Snowflake-workshop! De neste to timene skal vi bryne oss p√• innhenting, transformering og plotting av [tilsynsdata](https://hotell.difi.no/?dataset=mattilsynet/smilefjes/tilsyn) fra Digitaliseringsdirektoratet. Dette er en vurdering av over 3500 restauranter i Norge p√• parametre som lokaler, math√•ndtering, merking og lignende. V√•rt m√•l er √• ta i bruk geodata fra Kartverket for √• visualisere hvilke kommuner i Norge som har - og eventuelt _ikke_ har - restaurantene sine helt p√• stell üòÄ ü§î üò©

## DEL 1: Kobling mot Google Cloud Storage üíæ

Logg inn i [Snowflake](https://sc96841.europe-west4.gcp.snowflakecomputing.com/console/login#/) med brukernavn og passord du har blitt tildelt og naviger deg til **Projects -> Worksheets** og lag et nytt worksheet i h√∏yre hj√∏rne. N√• er du klar til √• utvikle i ditt eget arbeidsomr√•de!

> **Tips üí°** N√•r vi jobber i en Snowflake Worksheet er det ikke n√∏dvendig √• slette cellene etter de er kj√∏rt. Du kan heller markere de linjene du √∏nsker skal kj√∏re, s√• har du ogs√• historikken med deg til senere oppgaver.  

### Oppgave 1.1: Lag database og skjema 
Det f√∏rste du m√• gj√∏re er √• lage en egen database og et skjema (datasett) p√• formatet `ditt_navn`_database/schema slik som i kodesnutten under. Bytt med ditt eget navn og kj√∏r cellene i Snowflake. 

```sql
CREATE DATABASE ditt_navn_database; 
CREATE SCHEMA ditt_navn_schema;
```

Naviger deg til **Database** i panelet til venstre og kj√∏r refresh. N√• vil du forh√•pentligvis se at din nye database og skjema er opprettet.

<img src=https://github.com/bekk/snowflake-workshop/assets/29883261/6f4f1fac-94f7-43f5-94c0-6bd47b4e4a25 width=250 />

Det er slitsomt √• m√•tte spesifisere hele stien hver gang vi oppretter en tabell. Heldigvis kan du slippe dette ved √• sette hvilken kontekst du √∏nsker √• v√¶re i (alts√•, hvilken database og hvilket skjema du vil bruke). I filen kan du navigere deg i venstre hj√∏rne og sette database og skjema som vi akkurat lagde.

<img src=https://github.com/bekk/snowflake-workshop/assets/29883261/baac22e4-51e6-415c-98fb-02cf5ba46c44 width=400 />


Nok snikksnakk, la oss hente data fra GCP!

### Oppgave 1.2: Last inn data fra GCS-b√∏tte ü™£

N√• skal vi hente data fra `snowflake-ws-raw-data`-b√∏tta som ligger i [GCP](https://console.cloud.google.com/storage/browser?project=snowflake-workshop&prefix=&forceOnBucketsSortingFiltering=true). For √• gj√∏re dette er vi n√∏dt til √• opprette en konfigurasjonsenhet som brukes for √• integrere Snowflake med eksterne lagringstjenester (som Google Cloud Storage). Denne enheten kalles for `storage integration object` og oppretter blant annet en egen service account (maskinbruker) som vi kan gi tilgang til i b√∏tta v√•r. Kodesnutten under sier at vi √∏nsker √• lage et eksternt volum i GCS som har tilgang til en gitt sti. 

> **NB:** Det g√•r bare an √• sette opp √©n integrasjon per b√∏tte. Denne er allerede satt opp for dere, s√• dere trenger ikke kj√∏re kodesnutten under!

```sql
CREATE STORAGE INTEGRATION august_gcp_integration
    type = external_stage
    storage_provider = GCS 
    enabled = true 
    storage_allowed_locations = ('gcs://snowflake-ws-raw-data/');
```

N√• kan vi hente ut den genererte maskinbrukeren ved √• kj√∏re:

```sql
DESC STORAGE INTEGRATION august_gcp_integration;
```

Her ser vi at vi har f√•tt en maskinbruker, `STORAGE_GCP_SERVICE_ACCOUNT`, som vi kan gi tilgang til i b√∏tta. Dette er ogs√• allerede gjort, men du kan verifisere det ved √• navigere deg til [**Permissions**](https://console.cloud.google.com/storage/browser/snowflake-ws-raw-data;tab=permissions?forceOnBucketsSortingFiltering=true&project=snowflake-workshop&prefix=&forceOnObjectsSortingFiltering=false)-fanen i b√∏tta og se at maskinbrukeren har rettigheten `Storage Admin`.

S√• lett var det - n√• har vi muligheten til √• autentisere oss mot b√∏tta og hente ut dataen üöÄ

### Oppgave 1.3: Lag mellomledd for kildedata og datavarehus

Det siste vi trenger √• lage for √• hente data er et `stage object`. Dette er et omr√•de som brukes til midlertidig lagring av r√•datafiler f√∏r de lastes inn i Snowflake-databaser. Det fungerer som et mellomledd mellom kildedataene og datavarehuset v√•rt. For √• hente dataen m√• vi ta i bruk `storage integration`-objektet vi nettopp lagde ved √• kopiere og kj√∏re kodesnutten under:

```sql
CREATE STAGE gcp_data
    storage_integration = august_gcp_integration
    url = 'gcs://snowflake-ws-raw-data/';
```

Verifiser at dette funket ved √• kj√∏re `list @gcp_data;`. F√•r du opp fire filer, er vi _endelig_ klare til √• kopiere data inn i Snowflake. 


## DEL 2: Hent CSV-data for tilsyn og postnummer üì´

### Oppgave 2.1: Kopier data fra stage til tabell
N√• skal vi gj√∏re oss klare for √• laste inn data. F√∏rst er vi n√∏dt til √• lage et fil-format som matcher CSV-formatet. 
√Öpne `postnummer.csv` og `tilsyn.csv` i [b√∏tta](https://console.cloud.google.com/storage/browser/snowflake-ws-raw-data;tab=objects?forceOnBucketsSortingFiltering=true&project=snowflake-workshop&prefix=&forceOnObjectsSortingFiltering=false) for √• se hvilke verdier du m√• erstatte i kodesnutten under: 

```sql
CREATE FILE FORMAT csv_format
    type = csv 
    field_delimiter = <sett_inn_separasjonstegn>
    skip_header = <sett_inn_antall_rader_som_m√•_hoppes_over>;
```

I tillegg m√• vi initiere tabellen v√•r, `tilsyn`, med riktig antall kolonner og type. Dette er en d√∏ll prosess √• gj√∏re for h√•nd (ChatGPT fikset det for meg), s√• bare kopier og kj√∏r snutten under:

```sql
CREATE TABLE tilsyn (
    tilsynsobjektid VARCHAR,
    orgnummer VARCHAR,
    navn VARCHAR,
    adrlinje1 VARCHAR,
    adrlinje2 VARCHAR,
    postnr VARCHAR,
    poststed VARCHAR,
    tilsynid VARCHAR,
    sakref VARCHAR,
    status INT,
    dato VARCHAR,
    total_karakter INT,
    tilsynsbesoektype INT,
    tema1 VARCHAR,
    tema1_nn VARCHAR,
    karakter1 INT,
    tema2_no VARCHAR,
    tema2_nn VARCHAR,
    karakter2 INT,
    tema3_no VARCHAR,
    tema3_nn VARCHAR,
    karakter3 INT,
    tema4_no VARCHAR,
    tema4_nn VARCHAR,
    karakter4 INT
);
```

Til slutt kopierer vi dataen _fra_ stage-objektet v√•rt _til_ den nylig opprettede tabellen v√•r. Merk at n√•r vi tar i bruk stage-objektet v√•rt, s√• slipper vi ogs√• √• fore inn hele URL-en. Siden dataen ogs√• (muligens) inneholder noen korrupte rader, slenger vi p√• `on_error=continue` for at opplastingen skal hoppe over disse radene og fortsette innlastningen. Kj√∏r og kopier:

```sql
COPY INTO tilsyn 
FROM @gcp_data/csv/tilsyn.csv
file_format=csv_format
on_error=continue;
```

N√•r snutten er ferdigkj√∏rt kan vi se at det var seks rader som var feilformatert. Det bryr vi oss ikke s√• mye om i denne workshopen. 

Kj√∏r en sp√∏rring p√• tabellen for √• sjekke at dataen er korrekt lastet inn. 

### Oppgave 2.2: Transformer tilsynstabellen

Det er et par ting med den opprinnelige r√•dataen som ikke passer v√•rt form√•l. Lag derfor en sp√∏rring og skriv den til en ny tabell, `tilsyn_transformert`, som inneholder f√∏lgende:
1. `navn, orgnummer, postnr, poststed, total_karakter`-kolonnene p√• vanlig format
2. Sette verdier fra `karakter1` til kolonne `rutiner_og_ledelse`, `karakter2` til `lokaler_og_utstyr`, `karakter3` til `mathandtering_og_tilberedning` og `karakter4` til `merking_og_sporbarhet`
3. Omgj√∏re dato-strengen p√• formatet 'DDMMYYYY' til dato 
4. Kj√∏r en sp√∏rring som verifiserer at dataen ser korrekt ut.

<details>
  <summary>üö® L√∏sningsforslag</summary>
  
  ```sql
  CREATE TABLE tilsyn_transformert as 
      select 
          navn, 
          orgnummer,
          postnr, 
          poststed,
          total_karakter,
          karakter1 as rutiner_og_ledelse,
          karakter2 as lokaler_og_utstyr,
          karakter3 as mathandtering_og_tilberedning,
          karakter4 as merking_og_sporbarhet,
          TO_DATE(dato, 'DDMMYYYY') as dato
      from tilsyn;
  ```

</details>


### Oppgave 2.3: Hent inn postnummer-data

I neste del av workshopen skal vi f√• inn geodata for kommuner. Problemet v√•rt er at det eneste vi har √• g√• p√• i tilsynstabellen er `postnr`, mens kommune-dataen v√•r bare har info om kommunenavn og kommunenummer. Heldigvis har vi en fil i GCS, `postnummer`, som kan hjelpe oss med √• knytte de to tabellene sammen!

Vi √∏nsker √• gj√∏re akkurat det samme for `postnummer` som vi gjorde for tilsyn: 
1. Initiere tabellen med riktige antall kolonner og typer
2. Kopier fra stage-objekt til tabell
3. Verifiser resultatet

<details>
  <summary>üö® L√∏sningsforslag</summary>

```sql
-- Initier postnummer-tabell 
CREATE TABLE postnummer (
    postnummer VARCHAR,
    poststed VARCHAR,
    kommunenummer VARCHAR,
    kommunenavn VARCHAR
);

-- Kopier postnummer fra stage til Snowflake-tabell
COPY INTO postnummer 
FROM @gcp_data/csv/postnummer.csv
file_format=csv_format
on_error=continue;
```
  
</details>


## DEL 3: Hent og transformer JSON-data for kommuner üó∫Ô∏è

CSV-filene vi har jobbet med hittil har v√¶rt enkel, tabul√¶r data. Men hvordan h√•ndterer man semi-strukturert data som JSON?

### Oppgave 3.1: Last opp data fra stage til kommuner-tabell

Som i tidligere oppgaver m√• vi starte med √• initiere tabellen v√•r. I kodesnutten under lager vi tabellen `kommuner` med all data i √©n kolonne av typen `VARIANT`. Det er en fleksibel datatype som kan holde en hvilken som helst type av semi-strukturerte data som JSON, Avro, XML eller lignende. Kj√∏r kodesnutten under:

```sql
CREATE TABLE kommuner (
    json_data variant
);
```

Deretter m√• vi kopiere inn dataen fra GCS p√• samme m√•te som f√∏r. Det eneste vi dropper √• gj√∏re er √• lage et filformat slik som for CSV, da det eneste vi trenger √• sette er typen:

```sql
COPY INTO kommuner
from @gcp_data/json/kommuner.json
file_format = (type = JSON);
```

Ta en titt p√• tabellen vi n√• har opprettet. Her er vi n√∏dt til √• n√∏ste opp i et par ting üßπ

### Oppgave 3.2: Pakk ut JSON-data

La oss starte med √• pakke ut dataen slik at vi f√•r √©n kommune per rad. Vi bruker en noe mystisk funksjon, `LATERAL FLATTEN`, for √• pakke ut features-objektet. Det er den mest effektive m√•te √• pakke ut n√∏stede arrays i en JSON-kolonne, slik vi har her:

```sql
CREATE TABLE kommuner_unwrapped as 
    select f.value as feature
    from kommuner,
    lateral flatten(input => kommuner.json_data[0]:features) f;
```

Se p√• den nye tabellen v√•r. N√• har vi i alle fall √©n rad per feature (kommuner med data), men vi √∏nsker √• pakke ut dataen enda mer fra JSON-formatet til kolonner. 

Det vi trenger fra kommuner er kommunenavn, kommunenummer og geometri slik at vi kan sl√• det sammen med de andre tabellene og plotte tilsynskarakterene i et kart per kommune. Vi √∏nsker i samme slengen √• transformere geometry-objektet til bin√¶r-format, og det kan du gj√∏re ved √• bruke `ST_ASWKB(TRY_TO_GEOMETRY(feature:geometry))`. 

Finn ut av hvordan vi kan f√• transformert JSON-objektet til de √∏nskede kolonnene og pr√∏v selv!

<details>
  <summary>üö® L√∏sningsforslag</summary>


  I Snowflake aksesserer du JSON-objekter med kolon, `:`. Hvis du for eksempel har `{"key": "value"}` i en kolonne, `json_column`, s√• kan du hente ut verdien med `json_column:key::<TYPE>`, der `TYPE` er typen du √∏nsker √• konvertere til (eksempelvis `STRING`). For n√∏stede objekter kan du bare fortsette med den samme annotasjonen (eksempelvis `{ "outer": { "inner": "value" } }` blir `json_column:outer:inner::<TYPE>`). 

```sql
CREATE TABLE kommuner_transformert as
    select  
        feature:properties:kommunenavn::STRING as kommunenavn,
        feature:properties:kommunenummer::STRING as kommunenummer,
        ST_ASWKB(TRY_TO_GEOMETRY(feature:geometry)) AS geometry
    from kommuner_unwrapped;
```
  
</details>

### Oppgave 3.3: Sl√• sammen datasettene

N√• har vi all dataen vi trenger p√• formatet vi √∏nsker! Det siste vi da m√• gj√∏re er √• sl√• sammen datasettene. 
Som nevnt tidligere trenger vi postnummer-tabellen til √• knytte tilsynsdata og kommuner sammen. For √• oppn√• m√•let v√•rt trenger vi f√∏lgende kolonner: 

1. `navn`, `dato` og `total_karakter` fra `tilsyn_transformert`-tabellen
3. `kommunenavn`, `kommunenummer` og `geometry` fra `kommuner_transformert`-tabellen

Gj√∏r et fors√∏k selv!

> **Tips üí°** Dette kan fort bli en lang sp√∏rring, s√• det kan v√¶re lurt √• dele opp joins i hver sin delsp√∏rring ved bruk av `WITH <navn_p√•_delsp√∏rring> AS ...` og deretter bruke `<navn_p√•_delsp√∏rring>` i neste join. Du kan lese mer om `WITH`-clauses [her](https://www.geeksforgeeks.org/sql-with-clause/).

<details>
  <summary>üö® L√∏sningsforslag</summary>
    
```sql
CREATE TABLE tilsyn_med_kommune as (
  WITH tilsyn_med_postnummer as (
    SELECT 
      t.navn,
      t.dato,
      t.total_karakter,
      p.*
    FROM postnummer as p 
    JOIN tilsyn_transformert as t
    ON t.postnr = p.postnummer
  )
  SELECT 
    t.navn,
    t.dato,
    t.total_karakter, 
    t.postnummer,
    t.poststed,
    k.*
  FROM kommuner_transformert as k 
  LEFT JOIN tilsyn_med_postnummer as t
  ON k.kommunenummer = t.kommunenummer
);
```

</details>


Ta en titt p√• dataen n√•. N√• har vi egentlig all data vi trenger til √• plotte tilgjengelig!

## DEL 4: Grupper data p√• kommuner og plott resultatet üìä

### Oppgave 4.1: Karaktersnitt per kommune

Tabellen v√•r `tilsyn_med_kommune` har n√• √©n rad per tilsyn. Det vi n√• trenger √• gj√∏re er √• gruppere dataen slik at vi har en gjennomsnittskarakter p√• hver kommune. Lag en sp√∏rring som tar med `kommunenavn`, `geometry`, og gjennomsnittskarakteren av `total_karakter` (avrundet med tre desimaler). Kall den nye tabellen `tilsynskarakter_per_kommune`. 

<details>
  <summary>üö® L√∏sningsforslag</summary>

```sql
CREATE TABLE tilsynskarakter_per_kommune as 
    SELECT 
        kommunenavn,
        kommunenummer,
        geometry,
        round(avg(total_karakter), 3) as gjennomsnittlig_karakter
    FROM tilsyn_med_kommune
    GROUP BY kommunenavn, kommunenummer, geometry;
```
  
</details>

Kj√∏r en `SELECT` p√• den nye tabellen din og naviger deg til h√∏yre kolonne i resultatet. Scroller du nedover finner du mer detaljert info om `gjennomsnittlig_karakter`-kolonnen, som gjennomsnittlig karakter for alle kommuner, distribusjonen av karakterer og prosentvis null-verdier. Vi ser at kolonneverdiene er relativt normaldistribuert, s√• her blir det kult √• plotte!  


### Oppgave 4.2: Plott tilsynskarakter per kommune 
N√• skal vi ta i bruk dataene vi har laget via en snowflake-connector. En connector lar deg koble til Snowflake utenfor plattformen via f.eks en Python-applikasjon. Vi har allerede laget et Python-script for √• visualisere dataene i `tilsynskarakter_per_kommune`-tabellen. Scriptet konverterer geometrikolonnen tilbake til GeoJSON fra bin√¶rformat. Dette m√• til for √• kunne visualisere dataene i rammeverket `Folium`. Slik gj√∏r du det:

1. For √• kunne kj√∏re scriptet m√• du f√∏rst klone repoet
```sh
git clone git@github.com:bekk/snowflake-workshop.git
```

2. Av erfaring varierer Python noe fra maskin til maskin, s√• jeg anbefaler at dere lager et _virtual environment_ i repoet du nettopp klonet:
```sh
python3 -m venv .venv && source .venv/bin/activate
```

3. Deretter m√• du installere avhengighetene som trengs. Dette gj√∏r du ved √• navigere deg inn i `/visualisering` og kj√∏re
```sh
pip install -r requirements.txt
```

4. Etter √• ha installert avhengighetene m√• du sette inn de n√∏dvendige parameterene i `main.py`. Finn frem brukernavnet, passordet, database- og skjema-navnet. Fyll disse inn i `connector.connect`-funksjonen.

N√•r parametererne er ferdig utfylt kj√∏rer du f√∏lgende kommando i `/visualisering`-mappen

```sh
python main.py
```

Etter kommandoen er ferdigkj√∏rt vil det bli laget en fil `map.html`. √Öpne opp filen i en nettleser og du vil se dataene dine plottet p√• et kart.

Gratulerer - n√• kan du vite hvilke kommuner du b√∏r - og absolutt _ikke_ b√∏r - bes√∏ke om du er p√• jakt etter en kulinarisk opplevelse üçî

## DEL 5: Transformasjoner i dbt

Frem til n√• har vi kj√∏rt transformasjonene v√•re manuelt i et worksheet. Dette er ikke s√¶rlig skalerbart n√•r man jobber p√• prosjekt. Derfor √∏nsker vi √• ta i bruk **dbt**!

**dbt** (short for Data Build Tool) er et verkt√∏y som lar deg skrive SQL-transformasjoner som kode, organisere dem i en strukturert pipeline, og deretter kj√∏re og dokumentere dem. Med dbt kan du definere hvordan r√•data skal transformeres til analyseklar data ved √• bruke SQL og enkle YAML-konfigurasjoner.

N√• skal vi vise hvor lett det er √• komme i gang med dbt etter man allerede har transformasjonene man trenger!

### Oppgave 5.1: Sett opp dbt lokalt

Naviger deg til roten av prosjektet og last ned `dbt` for Snowflake:

```sh
pip install dbt-snowflake
```

Deretter initierer du dbt-prosjektet:

```sh
dbt init tilsyn_<ditt_navn>
```

N√• vil en grunnstruktur for prosjektet bli opprettet for deg, inkludert mapper og filer som dbt trenger for √• fungere. 

Det siste du trenger √• gj√∏re er √• fylle ut variablene du blir promtet til. Disse variablene blir lagret i en konfigurasjonsfil, `profiles.yml`, som brukes til √• definere hvordan dbt skal koble seg til databasen din i Snowflake. 
N√•r du har lagt inn alle variablene kan du √•pne filen (`~/.dbt/profiles.yml`). Da vil den se ca. slik ut:

```sh
tilsyn_<ditt_navn>:  # Dette m√• matche navnet i `dbt_project.yml`
  target: dev        # Standard milj√∏ som brukes
  outputs:           # Ulike milj√∏er kan konfigureres her
    dev:             # Dev-milj√∏et
      type: snowflake
      account: sc96841.europe-west4.gcp   # Snowflake-kontonavn (uten ".snowflakecomputing.com")
      user: <din_bruker>                  # Snowflake-brukernavn
      password: <ditt_passord>            # Snowflake-passord
      role: ACCOUNTADMIN                  # Rollen som skal brukes 
      database: <ditt_navn>_database      # Standard database
      warehouse: COMPUTE_WH               # Snowflake warehouse
      schema: <ditt_navn>_schema          # Standard skjema for milj√∏et
      threads: 4                          # Antall tr√•der som dbt kan bruke
```

Naviger deg til dbt-mappen (`tilsyn_<ditt_navn>`) og test forbindelsen med `dbt debug`.

### Oppgave 5.2: Konfigurer prosjektet 

N√• m√• vi oppdatere prosjektets konfigurasjonsfil, `dbt_project.yml`, for √• definere hvordan dbt skal h√•ndtere modeller, mapper, og hvilke skjemaer de ulike modellene skal lagres i.
Det eneste du trenger √• endre her er modellene:

```sh
models:
  tilsyn_project:      # Modeller i prosjektet
    staging:
      +schema: staging  # Bruker skjemaet "staging" for disse modellene
      +materialized: view  # Disse modellene blir lagret som views
    transformations:
      +schema: transformations
      +materialized: table  # Modellene her blir lagret som tabeller
    final:
      +schema: final
      +materialized: table
```

Dette forteller dbt hvilke mapper som inneholder hvilke typer modeller, og hvilke skjemaer de skal lagres i. Her har vi satt opp tre niv√•er for dataen v√•r:

- **staging**: R√•data fra med minimale mengder transformasjoner p√• dataen.
- **transformations**: Transformasjoner fra r√•data til tabeller som fortsatt ikke er aggregert.
- **final**: V√•rt finaliserte "gull"-datasett som inneholder ferdigknadd data.


### Oppgave 5.3: Konfigurer modellene

Vi √∏nsker √• strukturere modellene i mapper p√• samme m√•te som vi definerte de i prosjektkonfigurasjonen. Slett eksempel-mappen og lag tre nye:

```sh
mkdir -p models/staging models/transformations models/final
```

Det siste vi trenger er √• lage en fil for kildedataen v√•r under `models/sources.yml`. Dette gj√∏r det enkelt √• referere til kildedataene i modellene v√•re:

```sh
version: 2
sources:
  - name: raw
    schema: <ditt_navn>_schema
    tables:
      - name: tilsyn
      - name: postnummer
      - name: kommuner
```

Her tar vi utgangspunkt i de innlastede r√•dataene vi hentet i tidligere oppgaver, men det kunne like greit v√¶rt en kobling direkte mot staging-objektet v√•rt `gcp_data`. 

### Oppgave 5.4: Lag modellene

Endelig er vi klare til √• definere modellene v√•re! Ta utgangspunkt i koden du skrev og lag tilsvarende tabeller i dbt. Det vil alts√• si:

**staging**: `tilsyn`, `postnummer` og `kommuner`

**transformations**: `kommuner_transformert`, `tilsyn_transformert` og `tilsyn_med_kommune`

**final**: `tilsynskarakterer_per_kommune`

Merk at tabellene f√•r samme navn som filnavnet du bruker (eksempelvis `stg_tilsyn.sql` blir tabellen `stg_tilsyn` i Snowflake).

Noen tips f√∏r du starter:

> - _source_ brukes til √• referere til r√•data som er definert som kilder i dbt-prosjektet, alts√• de vi allerede har definert i `sources.yml`. Du refererer til kildetabellene med `{{ source('source_name','source_table') }}`
> - _ref_ brukes til √• referere til andre dbt-modeller i prosjektet. N√•r du bruker ref, sikrer dbt at avhengigheter mellom modellene h√•ndteres korrekt, slik at modellene kj√∏res i riktig rekkef√∏lge. Du bruker disse referansene ved `{{ ref( 'model_name' }}`

<details>
  <summary>üö® L√∏sningsforslag</summary>

Under `models/staging`:

```sql
# stg_tilsyn.sql
select * from {{ source('raw', 'tilsyn') }}
```

```sql
# stg_postnummer.sql
select * from {{ source('raw', 'postnummer') }}
```

```sql
# stg_kommuner.sql
select * from {{ source('raw', 'kommuner') }}
```

Under `models/transformation`:

```sql
# tilsyn_transformert.sql
select 
    navn, 
    orgnummer,
    postnr,
    poststed,
    total_karakter,
    karakter1 as rutiner_og_ledelse,
    karakter2 as lokaler_og_utstyr,
    karakter3 as mathandtering_og_tilberedning,
    karakter4 as merking_og_sporbarhet,
    to_date(dato, 'DDMMYYYY') as dato
from {{ ref('stg_tilsyn') }}
```

```sql
# kommuner_transformert.sql
select 
    feature:properties:kommunenavn::string as kommunenavn,
    feature:properties:kommunenummer::int as kommunenummer,
    st_aswkb(try_to_geometry(feature:geometry)) as geometry
from {{ ref('stg_kommuner') }}
```

```sql
# tilsyn_med_kommune.sql
with tilsyn_med_postnummer as (
    select 
        t.navn,
        t.dato,
        t.total_karakter,
        p.postnummer,
        p.poststed,
        p.kommunenummer
    from {{ ref('tilsyn_transformert') }} as t 
    join {{ ref('stg_postnummer') }} as p
    on t.postnr = p.postnummer        
)
select 
    tp.navn,
    tp.dato,
    tp.total_karakter,
    k.kommunenavn,
    k.kommunenummer,
    k.geometry
from tilsyn_med_postnummer as tp 
left join {{ ref('kommuner_transformert') }} as k 
on k.kommunenummer = tp.kommunenummer
```

Under `models/final`:

```sql
select 
    kommunenavn, 
    round(avg(total_karakter), 3) as gjennomsnittlig_karakter,
    geometry
from {{ ref('tilsyn_med_kommune') }}
group by kommunenavn, geometry
```

</details>


### Oppgave 5.5: Se det ferdige resultatet via dbt docs

Det siste vi n√• skal gj√∏re er √• se det endelige resultatet ved √• kj√∏re:

```sh
dbt docs generate
dbt docs serve
```

Trykk p√• lenken som blir generert og naviger deg ned til ikonet i h√∏yre hj√∏rnet. Da kan du se hele dataflyten v√•r visuelt:

![Skjermbilde 2025-01-27 kl  16 49 43](https://github.com/user-attachments/assets/61cdf0bd-27a3-4afe-aa29-c990bd734511) 


Voil√†! Ved hjelp av dbt har vi n√• f√•tt transformasjonene v√•re inn i kode üéâ Ikke bare det; vi har ogs√• _mye_ bedre kontroll p√• hvor dataen v√•r stammer fra, og hvilke transformasjoner som er gjort underveis.

dbt er et kraftig verkt√∏y som lar deg gj√∏re utrolig mange ting vi ikke rekker √• g√• gjennom i denne workshopen. Lek deg gjerne med flere dbt features hvis du har tid! 

